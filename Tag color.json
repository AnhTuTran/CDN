{"paragraphs":[{"text":"import org.apache.spark._\nimport java.io._\nimport scala.io.Source\n\n\n\tval textFile = sc.textFile(\"/home/ntm/Desktop/access_log/hcmut_access_2017.txt\")\n\n\tval regex = \"/.+\"\n\n\tval split_word = textFile.flatMap(line => line.split(\" \").filter(x => x.matches(regex))).map(word => (word,1.toLong)).reduceByKey(_+_)\n\n\tval split = split_word.sortBy(x => -x._2)\n\n\tval evalution = split_word.map(_._2).sum / textFile.count()\n\n\tval id_content_number = split.zipWithIndex().map{ case(line,count) => {(count+1).toString + \",\" + line }}\n\n\tval sepaRank = Array(22,4,21,100)\n\n\tvar count =0;\t\n\tvar posMH = -1\n\tvar posM = -1\n\tvar posML = -1\n\tval total01 = sepaRank(0) + sepaRank(1)\n\tval total012 = sepaRank(0) + sepaRank(1) + sepaRank(2)\n\tval total0123 = sepaRank(0) + sepaRank(1) + sepaRank(2) + sepaRank(3)\n\n\n\tval output = id_content_number.flatMap(line => line.split(\"\\n\").map( x => {\n\t\t\t\tcount  += 1;\n\t\t\t\tif ( count <= sepaRank(0))  x.concat(\",1111\");\n\t\t\t\telse if (count > sepaRank(0) && count <= total01) {\n\t\t\t\t\tposMH += 1\n\t\t\t\t\tvar xx = posMH % 4\n\t\t\t\t\txx match {\n\t\t\t\t\t\tcase 0 => x.concat(\",1110\");\n\t\t\t\t\t\tcase 1 => x.concat(\",1101\");\n\t\t\t\t\t\tcase 2 => x.concat(\",1011\");\n\t\t\t\t\t\tcase 3 => x.concat(\",0111\");\t\t\t    \n\t\t\t\t\t}\n\n\t\t\t\t}\t\n\t\t\t\telse if (count > total01 && count <= total012) {\n\t\t\t\t\tposM += 1\n\t\t\t\t\tvar xx = posM % 6\n\t\t\t\t\txx match {\n\t\t\t\t\t\tcase 0 => x.concat(\",1100\");\n\t\t\t\t\t\tcase 1 => x.concat(\",1010\");\n\t\t\t\t\t\tcase 2 => x.concat(\",1001\");\n\t\t\t\t\t\tcase 3 => x.concat(\",0110\");\n\t\t\t\t\t\tcase 4 => x.concat(\",0101\");\n\t\t\t\t\t\tcase 5 => x.concat(\",0011\");\n\t\t\t\t\t}\n\n\t\t\t\t}\n\t\t\t\telse if (count > total012 && count <= total0123) {\n\t\t\t\t\tposML += 1\n\t\t\t\t\tvar xx = posML % 4\n\t\t\t\t\txx match {\n\t\t\t\t\t\tcase 0 => x.concat(\",1000\");\n\t\t\t\t\t\tcase 1 => x.concat(\",0100\");\n\t\t\t\t\t\tcase 2 => x.concat(\",0010\");\n\t\t\t\t\t\tcase 3 => x.concat(\",0001\");\t\t\t    \n\t\t\t\t\t}\n\t\t\t\t}\t\t\t\t\t\t\t\t\n\t\t\t\telse  x.concat(\",0000\");\t\t\n\t\t\t}))\n\n\n\toutput.coalesce(1,true).saveAsTextFile(\"/home/ntm/Desktop/output11\")\n\t\t\n","user":"anonymous","dateUpdated":"2018-03-13T14:33:01+0700","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala","editorHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1520308545469_-169068213","id":"20180306-105545_993799470","dateCreated":"2018-03-06T10:55:45+0700","dateStarted":"2018-03-13T14:33:01+0700","dateFinished":"2018-03-13T14:37:17+0700","status":"ERROR","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:5072","results":{"code":"ERROR","msg":[{"type":"TEXT","data":"import org.apache.spark._\nimport java.io._\nimport scala.io.Source\ntextFile: org.apache.spark.rdd.RDD[String] = /home/ntm/Desktop/access_log/hcmut_access_2017.txt MapPartitionsRDD[753] at textFile at <console>:345\nregex: String = /.+\nsplit_word: org.apache.spark.rdd.RDD[(String, Long)] = ShuffledRDD[756] at reduceByKey at <console>:388\nsplit: org.apache.spark.rdd.RDD[(String, Long)] = MapPartitionsRDD[761] at sortBy at <console>:390\nevalution: Double = 0.9745034374805687\nid_content_number: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[765] at map at <console>:392\nsepaRank: Array[Int] = Array(22, 4, 21, 100)\ncount: Int = 0\nposMH: Int = -1\nposM: Int = -1\nposML: Int = -1\ntotal01: Int = 26\ntotal012: Int = 47\ntotal0123: Int = 147\noutput: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[766] at flatMap at <console>:411\norg.apache.hadoop.mapred.FileAlreadyExistsException: Output directory file:/home/ntm/Desktop/output11 already exists\n  at org.apache.hadoop.mapred.FileOutputFormat.checkOutputSpecs(FileOutputFormat.java:131)\n  at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply$mcV$sp(PairRDDFunctions.scala:1119)\n  at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.scala:1096)\n  at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.scala:1096)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n  at org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n  at org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1096)\n  at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply$mcV$sp(PairRDDFunctions.scala:1070)\n  at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1035)\n  at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1035)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n  at org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n  at org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1035)\n  at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply$mcV$sp(PairRDDFunctions.scala:961)\n  at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply(PairRDDFunctions.scala:961)\n  at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply(PairRDDFunctions.scala:961)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n  at org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n  at org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:960)\n  at org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply$mcV$sp(RDD.scala:1489)\n  at org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply(RDD.scala:1468)\n  at org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply(RDD.scala:1468)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n  at org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n  at org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1468)\n  ... 150 elided\n"}]}},{"text":"","user":"anonymous","dateUpdated":"2018-03-06T16:09:22+0700","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala"},"tableHide":false,"editorHide":false,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1520309469183_-148803525","id":"20180306-111109_441864714","dateCreated":"2018-03-06T11:11:09+0700","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:5074"}],"name":"log","id":"2D7XSUWMF","angularObjects":{"2D7F5WZ3Y:shared_process":[],"2D7GMNNGQ:shared_process":[],"2D7GSW1AW:shared_process":[],"2D9C611XK:shared_process":[],"2D8XS5NZK:shared_process":[],"2DAC6TT6G:shared_process":[],"2D93XZNV7:shared_process":[],"2D84T2E7F:shared_process":[],"2DAAN1PZW:shared_process":[],"2D93ZAZ99:shared_process":[],"2DA3RJ3HV:shared_process":[],"2D8F1CDRV:shared_process":[],"2D7N6J63A:shared_process":[],"2D7343SYH:shared_process":[],"2D9XB5X1M:shared_process":[],"2DAEXMG5R:shared_process":[],"2D9ST5PKZ:shared_process":[],"2D71VY8MX:shared_process":[],"2D9MPEZKV:shared_process":[]},"config":{"looknfeel":"default","personalizedMode":"false"},"info":{}}